{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bf9025-63ea-40b6-8d9a-a7ea57c65fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\seaborn\\relational.py:17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_oldcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     VectorPlotter,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     locator_to_legend_entries,\n\u001b[0;32m     13\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     14\u001b[0m     _default_color,\n\u001b[0;32m     15\u001b[0m     _deprecate_ci,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\seaborn\\_statistics.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     32\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper,\n\u001b[0;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\spatial\\__init__.py:111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_procrustes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m procrustes\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_geometric_slerp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geometric_slerp\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ckdtree, kdtree, qhull\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\spatial\\_geometric_slerp.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m euclidean\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\spatial\\distance.py:120\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_validated\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distance_wrap\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _hausdorff\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rel_entr\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torchvision\n",
    "import torcheval\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918fc7c-3071-4bb7-9e14-94432a1899d0",
   "metadata": {},
   "source": [
    "# Работа с исходными данными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e22822-6968-49fa-ab26-f033874731a7",
   "metadata": {},
   "source": [
    "## Обработка метаданных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154c648-617e-4077-b890-70e4805b1135",
   "metadata": {},
   "source": [
    "Загрузим метаданные о датасете и данные о диагнозах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36053b3-4a1b-4884-be31-81ad54206fa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv('ptbxl_database.csv', index_col=0)\n",
    "scp_df = pd.read_csv('scp_statements.csv')\n",
    "\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312443c-a9d9-4432-a208-6922505601fe",
   "metadata": {},
   "source": [
    "Отфильтруем данные.\n",
    "\n",
    "В метаданных нужно оставить столбцы, включающие в себя диагноз, путь к файлу с временным рядом, разделения на 10 групп. Обработаем столбец с заключениями в соответствии с синтаксисом словаря.\n",
    "\n",
    "В данных о диагнозах оставим только диагностические заключения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82af7df-e4fb-440c-8f2c-594a3f832084",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.scp_codes = meta_df.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "meta_df = meta_df[['scp_codes', 'strat_fold', 'filename_lr']]\n",
    "\n",
    "\n",
    "scp_df = scp_df[scp_df.diagnostic == 1]\n",
    "diagnose_set = set(scp_df['Unnamed: 0'])\n",
    "\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd29fc-b173-4c46-b7fa-ed18184ad7ef",
   "metadata": {},
   "source": [
    "## Загрузка временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6797040-fb3c-483d-9341-31e6cc477aa9",
   "metadata": {},
   "source": [
    "Для дальнейшего анализа будем использовать записи с частотой 100 измерений в секунду. Загрузим записи в numpy массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f732cf-91c5-4351-9c81-3308cd6b0ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "\n",
    "path = ''\n",
    "sampling_rate=100\n",
    "\n",
    "X = load_raw_data(meta_df, sampling_rate, path)\n",
    "X = X.transpose(0,2,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a47873-bf90-4bcc-b1df-6f3e705cf0ee",
   "metadata": {},
   "source": [
    "Размер полученных данных соответствует 21837 случаям, для каждого проведено 12 одновременных измерений по 10*100=1000 точек\n",
    "\n",
    "Визуализируем одно из ЭКГ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa82af-ba76-4f5c-8e7e-f29b07bad795",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X[45]\n",
    "\n",
    "fig, axes = plt.subplots(12, 1, figsize=(20,10), sharex=True)\n",
    "for i in range(12):\n",
    "    sns.lineplot(x=np.arange(sample.shape[1]), y=sample[i, :], ax=axes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e53d3-5769-4d65-a7f3-d79a7c238c7d",
   "metadata": {},
   "source": [
    "## Разметка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841a56-a709-4030-83b6-a8fc6b6e220e",
   "metadata": {},
   "source": [
    "Разметим ЭКГ, будем использовать только диагностические заметки, уровень будет равен степени уверенности в диагнозе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a90dfc-ebdb-4a7f-9778-6fe9ba10aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marking(labels, classes):\n",
    "    # Словарь для индексов классов\n",
    "    classes_dict = {key: value for key, value in zip(classes, range(len(classes)))}\n",
    "    res = np.zeros([len(labels), len(classes)])\n",
    "    # Пройдёмся по каждому диагнозу, в нём просматриваем словарь, проверяем входит ли запись в диагнозы и, если да, то записываем значение в пределах [0,1]\n",
    "    for i, ecg in enumerate(labels):\n",
    "        for key, value in ecg.items():\n",
    "            if key in classes_dict:\n",
    "                j = classes_dict[key]\n",
    "                res[i,j] = value/100\n",
    "    return res    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf5c5b-d1ef-46b9-8a26-7df8a4936c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = marking(meta_df.scp_codes, diagnose_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d69bc1-1544-4106-9c0b-c481bd49272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = [10]\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for i in range(len(meta_df)):\n",
    "    if Y[i].sum() == 0:\n",
    "        continue\n",
    "    if meta_df.iloc[i]['strat_fold'] in test_fold:\n",
    "        X_test.append(X[i])\n",
    "        Y_test.append(Y[i])\n",
    "    else:\n",
    "        X_train.append(X[i])\n",
    "        Y_train.append(Y[i])\n",
    "    print(f\"Done {i} of {len(meta_df)}\", end='\\r')\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c53fd7-f140-43ac-bdb3-960e8856f196",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8921202-6278-4d6e-8657-219e3c8acb0d",
   "metadata": {},
   "source": [
    "#### Определение функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda7b6c-1837-4e46-95f6-c2d6b0144995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_classes_row(data, name):\n",
    "    count = np.zeros(len(diagnose_set))\n",
    "    for row in data:\n",
    "        num = (row > 0).sum()\n",
    "        count[num] += 1\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(np.arange(len(diagnose_set)), count)\n",
    "    plt.title(f\"Распределение количества диагнозов по ЭКГ в {name} датасете\")\n",
    "    plt.show()\n",
    "    print({num: cnt for num, cnt in zip(range(len(count)), count)})\n",
    "\n",
    "\n",
    "def num_classes_dataset(data, name):\n",
    "    count = (data > 0).sum(axis=0)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(list(diagnose_set), count)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(f\"Распределение частоты диагнозов в {name} датасете\")\n",
    "    plt.show()\n",
    "\n",
    "def sum_coef_classes_dataset(data, name):\n",
    "    count = data.sum(axis=0)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(list(diagnose_set), count)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(f\"Распределение диагнозов, учитывая стпень уверенности в {name} датасете\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def avg_coef_classes_dataset(data, name):\n",
    "    count1 = data.sum(axis=0)\n",
    "    count2 = (data > 0).sum(axis=0)\n",
    "    avg_count = count1 / count2\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(list(diagnose_set), avg_count)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(f\"Распределение средней уверенности в диагнозе в {name} датасете\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6f545-df8f-4249-863d-5bbe596db6ae",
   "metadata": {},
   "source": [
    "#### Анализ обучающего и тестового датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258ad6d-6826-45eb-8484-886d3507aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_row(Y_train, 'обучающем')\n",
    "num_classes_row(Y_test, 'тестовом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c911d78-7336-4f9d-9546-70a5d727b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_coef_classes_dataset(Y_train, 'обучающем')\n",
    "sum_coef_classes_dataset(Y_test, 'тестовом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5124a03-286c-467b-9def-8ecf8dfbf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_dataset(Y_train, 'обучающем')\n",
    "num_classes_dataset(Y_test, 'тестовом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b7a9c-e1dd-4557-b37a-2b62e074af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_coef_classes_dataset(Y_train, 'обучающем')\n",
    "avg_coef_classes_dataset(Y_test, 'тестовом')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7f12a-328c-4137-b067-1e7137b7a194",
   "metadata": {},
   "source": [
    "## Балансировка обучающего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac3542-4ff8-4365-b90b-73e5c72c10a0",
   "metadata": {},
   "source": [
    "#### Определение авторского метода балансировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22e7cb-a012-428e-af32-91949eb44788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob(x, threshold_inf):\n",
    "    if x <= threshold_inf:\n",
    "        return 0\n",
    "    return 1 / (1 + math.exp(-math.log(x)))\n",
    "\n",
    "\n",
    "def get_information(values, distribution_0):\n",
    "    arr = np.zeros(len(values))\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = -math.log(abs(values[i] - 1/distribution_0[i]))        \n",
    "    return np.sqrt(np.mean(arr**2))\n",
    "\n",
    "\n",
    "def multiply(dataX, dataY, threshold_i, threshold_p):\n",
    "    distribution0 = 1 - (dataY > 0).sum(axis=0)/len(dataY)\n",
    "    information = [get_information(row, distribution0) for row in dataY]\n",
    "    multiply_probability = [Prob(information_value,threshold_i) for information_value in information]\n",
    "    dataY = list(dataY)\n",
    "    dataX = list(dataX)\n",
    "    for i in range(len(dataY)):\n",
    "        if random.random() < multiply_probability[i] and multiply_probability[i] > threshold_p:\n",
    "            dataY.append(dataY[i])\n",
    "            dataX.append(dataX[i])\n",
    "    return (np.array(dataX), np.array(dataY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f4904-8570-455a-8e9d-48a46bde5c7c",
   "metadata": {},
   "source": [
    "#### Определение MLSMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02bce6-c1a7-477e-a062-33276e9ffb7b",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/tolgadincer/upsampling-multilabel-data-with-mlsmote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495f983-c7da-4b6c-ab50-4783b2493bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_sample=1000):\n",
    "    ''' \n",
    "    Create a unevenly distributed sample data set multilabel  \n",
    "    classification using make_classification function\n",
    "    \n",
    "    args\n",
    "    nsample: int, Number of sample to be created\n",
    "    \n",
    "    return\n",
    "    X: pandas.DataFrame, feature vector dataframe with 10 features \n",
    "    y: pandas.DataFrame, target vector dataframe with 5 labels\n",
    "    '''\n",
    "    X, y = make_classification(n_classes=5, class_sep=2,\n",
    "                               weights=[0.1,0.025, 0.205, 0.008, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "                               n_features=10, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    y = pd.get_dummies(y, prefix='class')\n",
    "    return pd.DataFrame(X), y\n",
    "\n",
    "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
    "    \"\"\"\n",
    "    Find the underrepresented targets.\n",
    "    Underrepresented targets are those which are observed less than the median occurance.\n",
    "    Targets beyond a quantile limit are filtered.\n",
    "    \"\"\"\n",
    "    irlbl = df.sum(axis=0)\n",
    "    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    return tail_label\n",
    "\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
    "    \"\"\"\n",
    "    return\n",
    "    X_sub: pandas.DataFrame, the feature vector minority dataframe\n",
    "    y_sub: pandas.DataFrame, the target vector minority dataframe\n",
    "    \"\"\"\n",
    "    tail_labels = get_tail_label(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    \n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    \"\"\"\n",
    "    Give index of 10 nearest neighbor of all the instance\n",
    "    \n",
    "    args\n",
    "    X: np.array, array whose nearest neighbor has to find\n",
    "    \n",
    "    return\n",
    "    indices: list of list, index of 5 NN of each element in X\n",
    "    \"\"\"\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "def MLSMOTE(X, y, n_sample, neigh=5):\n",
    "    \"\"\"\n",
    "    Give the augmented data using MLSMOTE algorithm\n",
    "    \n",
    "    args\n",
    "    X: pandas.DataFrame, input vector DataFrame\n",
    "    y: pandas.DataFrame, feature vector dataframe\n",
    "    n_sample: int, number of newly generated sample\n",
    "    \n",
    "    return\n",
    "    new_X: pandas.DataFrame, augmented feature vector data\n",
    "    target: pandas.DataFrame, augmented target vector data\n",
    "    \"\"\"\n",
    "    indices2 = nearest_neighbour(X, neigh=5)\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    target = np.zeros((n_sample, y.shape[1]))\n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0, n-1)\n",
    "        neighbor = random.choice(indices2[reference, 1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)\n",
    "        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference,:] - X.loc[neighbor,:]\n",
    "        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n",
    "    new_X = pd.DataFrame(new_X, columns=X.columns)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    return new_X, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8e9b7-2daa-4d0d-9c04-924ddd217203",
   "metadata": {},
   "source": [
    "#### Балансировка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd84fab-bcbe-4180-a0fc-2f771b6d8253",
   "metadata": {},
   "source": [
    "#### Авторский метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3143c4-ab3f-4651-b1bc-e5999e77a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_balanced = Y_train\n",
    "X_train_balanced = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aefdc0-4451-4c10-a87f-3d5d522cda43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_balanced.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f570ca0-610b-4147-99e8-503d1e466133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('изначальное распределение количества классов для примера %s' % Counter(Y_train_balanced.sum(axis=1)))\n",
    "print('изначальное распределение вероятности классов', *Y_train_balanced.sum(axis=0)/len(Y_train_balanced))\n",
    "print('изначальное распределение редкости классов', *max(Y_train_balanced.sum(axis=0))/Y_train_balanced.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e158e4-fed4-4259-8738-fb40910fb0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    X_train_balanced, Y_train_balanced = multiply(X_train_balanced, Y_train_balanced, 0.9, 0.1)\n",
    "    print('\\n')\n",
    "    print(f\"Длина датасета на шаге {i+1} {len(Y_train_balanced)}\")\n",
    "    print(f'Распределение редкости классов на шаге {i+1} {max(Y_train_balanced.sum(axis=0))/Y_train_balanced.sum(axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4590f4a-d195-4422-a61b-c55ece6f7694",
   "metadata": {},
   "source": [
    "#### MLSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fafa9d-dbdc-41d2-9dbc-7cd5ff7e4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "ter = torch.tensor(X_train, dtype=torch.float32)\n",
    "flat = torch.flatten(ter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccec02-e958-4af9-b195-c782226b6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(flat)\n",
    "b = pd.DataFrame(Y_train > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad47a0c-16bf-44ab-97f0-67b822b94df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, y_sub = get_minority_samples(a, b)  # Getting minority samples of that datframe\n",
    "X_res, y_res = MLSMOTE(X_sub, y_sub, 10000, 10)  # Applying MLSMOTE to augment the dataframe\n",
    "\n",
    "X_res = np.array(X_res)\n",
    "y_res = np.array(y_res)\n",
    "\n",
    "X_res = torch.tensor(X_res, dtype=torch.float32)\n",
    "X_res = X_res.view(-1, 12, 1000)\n",
    "X_res = np.array(X_res)\n",
    "\n",
    "X_train_balanced_MLSMOTE = np.concatenate((X_train, X_res), axis=0)\n",
    "Y_train_balanced_MLSMOTE = np.concatenate((Y_train, y_res), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad9203-60db-484d-a8ec-cee5fa28d906",
   "metadata": {},
   "source": [
    "#### Результат балансировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce873df-fd19-475d-adab-3effa5f0aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_dataset(Y_train_balanced, 'сбалансированном авторском обучающем')\n",
    "num_classes_dataset(Y_train_balanced_MLSMOTE, 'сбалансированном MLSMOTE обучающем')\n",
    "num_classes_dataset(Y_train, 'исходном обучающем')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4701f-5bdb-4ec0-80dd-715278849ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_row(Y_train_balanced_MLSMOTE, 'сбалансированном MLSMOTE обучающем')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa867da7-f983-41da-afe6-cbd62c696502",
   "metadata": {},
   "source": [
    "## Подготовка моделей классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5c554-7637-4a3b-a52d-c0683b734e42",
   "metadata": {},
   "source": [
    "### Подготовка загрузчиков данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692b605-1da6-4dd3-879f-b713e40fcc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train1 = Y_train > 0\n",
    "Y_train_balanced1 = Y_train_balanced > 0\n",
    "Y_train_balanced_MLSMOTE1 = Y_train_balanced_MLSMOTE > 0\n",
    "Y_test1 = Y_test > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203b78e-8231-41a3-ae83-e11c66f559ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.transpose(0,2,1)\n",
    "X_train_balanced1 = X_train_balanced.transpose(0,2,1)\n",
    "X_train_balanced_MLSMOTE1 = X_train_balanced_MLSMOTE.transpose(0,2,1)\n",
    "X_test1 = X_test.transpose(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946403d-e51e-46a0-b9d9-3509c834131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = torch.tensor(X_train1, dtype=torch.float32)\n",
    "Y_train1 = torch.tensor(Y_train1, dtype=torch.float32)\n",
    "X_train_balanced1 = torch.tensor(X_train_balanced1, dtype=torch.float32)\n",
    "Y_train_balanced1 = torch.tensor(Y_train_balanced1, dtype=torch.float32)\n",
    "X_train_balanced_MLSMOTE1 = torch.tensor(X_train_balanced_MLSMOTE1, dtype=torch.float32)\n",
    "Y_train_balanced_MLSMOTE1 = torch.tensor(Y_train_balanced_MLSMOTE1, dtype=torch.float32)\n",
    "X_test1 = torch.tensor(X_test1, dtype=torch.float32)\n",
    "Y_test1 = torch.tensor(Y_test1, dtype=torch.float32)\n",
    "\n",
    "batch = 256\n",
    "\n",
    "Train_basic = torch.utils.data.DataLoader(tuple(zip(X_train1, Y_train1)), batch_size=batch, shuffle=True)\n",
    "Train_balanced = torch.utils.data.DataLoader(tuple(zip(X_train_balanced1, Y_train_balanced1)), batch_size=batch, shuffle=True)\n",
    "Train_balanced_MLSMOTE = torch.utils.data.DataLoader(tuple(zip(X_train_balanced_MLSMOTE1, Y_train_balanced_MLSMOTE1)), batch_size=batch, shuffle=True)\n",
    "Test = torch.utils.data.DataLoader(tuple(zip(X_test1, Y_test1)), batch_size=batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58f8d0-f299-4f76-b3a2-12bea792c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_arr = [accuracy_score, precision_score, f1_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3ea94-3085-4a98-8dda-47c861188af4",
   "metadata": {},
   "source": [
    "### Функция обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115b2f8-8180-4fcd-834c-d71b388d162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, train, test, epoch_num, metrics_threshold, label,\n",
    "                metric_funcs, verbose=False, plot=True):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "    \n",
    "    hist_train = np.zeros(epoch_num)\n",
    "    hist_test = np.zeros(epoch_num)\n",
    "    metrics_train = np.zeros((epoch_num, len(metric_funcs), len(diagnose_set)))\n",
    "    metrics_test = np.zeros((epoch_num, len(metric_funcs), len(diagnose_set)))\n",
    "    \n",
    "    \n",
    "    for i in range(epoch_num):      \n",
    "        \n",
    "        hist_train_epoch = 0\n",
    "        Pred_train = np.empty((1, len(diagnose_set)))\n",
    "        Real_train = np.empty((1, len(diagnose_set)))\n",
    "        \n",
    "        model.train()\n",
    "        for j, (X, Y) in enumerate(train): \n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Y_pred = model(X)\n",
    "            loss = criterion(Y_pred, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            hist_train_epoch += loss.item()\n",
    "            Y_threshold = torch.sigmoid(Y_pred) >= metrics_threshold\n",
    "            Pred_train = np.concatenate((Pred_train, Y_threshold.cpu().detach().numpy()))\n",
    "            Real_train = np.concatenate((Real_train, Y.cpu().detach().numpy()))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Пройден обучающий батч {j} из {len(train)}\", end='\\r')\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "        hist_test_epoch = 0\n",
    "        Pred_test = np.empty((1, len(diagnose_set)))\n",
    "        Real_test = np.empty((1, len(diagnose_set)))\n",
    "        \n",
    "        model.eval()\n",
    "        for j, (X, Y) in enumerate(test):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)            \n",
    "            Y_pred = model(X)\n",
    "            loss = criterion(Y_pred, Y)\n",
    "\n",
    "            hist_test_epoch += loss.item()\n",
    "            Y_threshold = torch.sigmoid(Y_pred) >= metrics_threshold\n",
    "            Pred_test = np.concatenate((Pred_test, Y_threshold.cpu().detach().numpy()))\n",
    "            Real_test = np.concatenate((Real_test, Y.cpu().detach().numpy()))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Пройден тестовый батч {j} из {len(test)}\", end='\\r')\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        Pred_train = Pred_train[1:]\n",
    "        Pred_test = Pred_test[1:]\n",
    "        Real_train = Real_train[1:]\n",
    "        Real_test = Real_test[1:]\n",
    "\n",
    "        for j, metric in enumerate(metric_funcs):\n",
    "            for k, (pred_col, trg_col) in enumerate(zip(Pred_train.T, Real_train.T)):\n",
    "                metrics_train[i,j,k] = metric(pred_col, trg_col)\n",
    "\n",
    "        for j, metric in enumerate(metric_funcs):\n",
    "            for k, (pred_col, trg_col) in enumerate(zip(Pred_test.T, Real_test.T)):\n",
    "                metrics_test[i,j,k] = metric(pred_col, trg_col)\n",
    "        \n",
    "\n",
    "        hist_train[i] = hist_train_epoch/len(train)\n",
    "        hist_test[i] = hist_test_epoch/len(test)\n",
    "\n",
    "        if verbose:\n",
    "            if (i) % verbose == 0:\n",
    "                print(f\"Пройдена эпоха {i+1} из {epoch_num}, результаты:\")\n",
    "                print(f\"Loss Train / Test: {hist_train[i]} / {hist_test[i]}\")\n",
    "                for j, metric in enumerate(metric_funcs):\n",
    "                    tmp_dict_train = {key: value for key, value in zip(diagnose_set, metrics_train[i, j, :])}\n",
    "                    tmp_dict_test = {key: value for key, value in zip(diagnose_set, metrics_test[i, j, :])}\n",
    "                    print(f\"Обучающая выборка - значения метрики {metric.__name__} на итерации {i+1} = \\n {tmp_dict_train}\")\n",
    "                    print(f\"Тестовая выборка - значения метрики {metric.__name__} на итерации {i+1} = \\n {tmp_dict_test}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(16, 6))        \n",
    "        plt.plot(hist_test, label=f\"Loss test {label}\")\n",
    "        plt.plot(hist_train, label=f\"Loss train {label}\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "    return (hist_train, hist_test, metrics_train, metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8c29f-1de5-4893-867a-0bca4841936e",
   "metadata": {},
   "source": [
    "### Рекурентная GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625f912-ce69-42a6-8596-5ba1ea993cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_gru(torch.nn.Module):\n",
    "    def __init__(self, hidden, layers):\n",
    "        super().__init__()\n",
    "        self.gru2 = torch.nn.GRU(input_size=12, hidden_size=hidden, batch_first=True, bidirectional=True, num_layers=layers, dropout=0.25)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.head = torch.nn.Linear(layers*2*hidden, len(diagnose_set))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        _, s3 = self.gru2(X)\n",
    "        flat = self.flatten(torch.transpose(s3, 0, 1))\n",
    "        out = self.head(flat)\n",
    "        return out\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0f001-96c8-4196-90a7-7ce24395949a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_gru_basic = Network_gru(64, 4)\n",
    "recurent_res_basic = train_model(model_gru_basic, 0.001, Train_basic, Test, 40, 0.5, 'recurrent basic', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491617d-6774-44d6-8ba8-793101de502d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_gru_balanced = Network_gru(64, 4)\n",
    "recurent_res_balanced = train_model(model_gru_balanced, 0.001, Train_balanced, Test, 25, 0.5, 'recurrent balanced', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6e907-daca-4819-ac39-b844c0d10d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_gru_balanced_MLSMOTE = Network_gru(64, 4)\n",
    "recurent_res_balanced_MLSMOTE = train_model(model_gru_balanced_MLSMOTE, 0.001, Train_balanced_MLSMOTE, Test, 25, 0.5, 'recurrent balanced SMOTE', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ae36b-956a-46c7-a98c-40578b7ed458",
   "metadata": {},
   "source": [
    "### Свёрточная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e888f1-d2e4-4d03-b043-0ca23c7611ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train2 = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_train_balanced2 = torch.tensor(X_train_balanced, dtype=torch.float32)\n",
    "X_train_balanced_MLSMOTE2 = torch.tensor(X_train_balanced_MLSMOTE, dtype=torch.float32)\n",
    "X_test2 = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "batch = 256\n",
    "\n",
    "Train_basic_conv = torch.utils.data.DataLoader(tuple(zip(X_train2, Y_train1)), batch_size=batch, shuffle=True)\n",
    "Train_balanced_conv = torch.utils.data.DataLoader(tuple(zip(X_train_balanced2, Y_train_balanced1)), batch_size=batch, shuffle=True)\n",
    "Train_balanced_MLSMOTE_conv = torch.utils.data.DataLoader(tuple(zip(X_train_balanced_MLSMOTE2, Y_train_balanced_MLSMOTE1)), batch_size=batch, shuffle=True)\n",
    "Test_conv = torch.utils.data.DataLoader(tuple(zip(X_test2, Y_test1)), batch_size=batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716644bc-a1ff-4612-a271-d7642ba8b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network_basic_conv = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(12, 48, 7, padding=3, stride=3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    torch.nn.Conv1d(48, 128, 5, padding=2, stride=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    torch.nn.Conv1d(128, 256, 3, padding=1, stride=1),\n",
    "    torch.nn.MaxPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    torch.nn.Conv1d(256, 512, 3, padding=1, stride=1),\n",
    "    torch.nn.AvgPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    torch.nn.Conv1d(512, 128, 5, stride=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(1024, len(diagnose_set))\n",
    ")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "torch.save(Network_basic_conv, buffer) \n",
    "print(buffer.tell()) \n",
    "\n",
    "del Network_basic_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a2625-19a5-4731-873f-f40f75f895e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "model_conv_basic = torch.load(buffer)\n",
    "\n",
    "conv_res_basic = train_model(model_conv_basic, 0.001, Train_basic_conv, Test_conv, 40, 0.5, 'convolutional basic', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e51f2d-4839-4661-92e4-1a3f18973feb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "model_conv_balanced = torch.load(buffer)\n",
    "\n",
    "conv_res_balanced = train_model(model_conv_balanced, 0.001, Train_balanced_conv, Test_conv, 25, 0.5, 'convolutional balanced', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7949fa5-d32f-48dc-b6bc-cf2bbb306685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "model_conv_balanced_MLSMOTE = torch.load(buffer)\n",
    "\n",
    "conv_res_balanced_MLSMOTE = train_model(model_conv_balanced_MLSMOTE, 0.001, Train_balanced_MLSMOTE_conv, Test_conv, 25, 0.5, 'convolutional balanced SMOTE', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c202-1097-45a2-bee8-d601858c3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "del buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13c0f2-fee3-4fa7-9ab9-7ae8f7c7fa2d",
   "metadata": {},
   "source": [
    "### Свёрточная со skip connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed066ab9-b4d5-4571-af20-50f6a85ca701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_block_1D(torch.nn.Module):\n",
    "    def __init__(self, c1, c2, c3, c4):\n",
    "        super().__init__()\n",
    "        self.forw = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(c1, c2, 3, padding=1),\n",
    "            torch.nn.BatchNorm1d(c2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv1d(c2, c3, 3, padding=1),\n",
    "            torch.nn.BatchNorm1d(c3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv1d(c3, c4, 3, padding=1),\n",
    "            torch.nn.BatchNorm1d(c4)\n",
    "        )\n",
    "        self.residual = torch.nn.Conv1d(c1, c4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        forw = self.forw(x)\n",
    "        residual = self.residual(x)\n",
    "        return forw + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646876b-f165-4d67-a834-b64ea3d8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "Network_residual_conv = torch.nn.Sequential(\n",
    "    resnet_block_1D(12, 24, 36, 48),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    resnet_block_1D(48, 72, 100, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    resnet_block_1D(128, 160, 200, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    resnet_block_1D(256, 320, 400, 512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool1d(3, stride=2),\n",
    "    torch.nn.Dropout1d(0.25),\n",
    "    resnet_block_1D(512, 680, 850, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.AvgPool1d(61),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(1024, len(diagnose_set))\n",
    ")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "torch.save(Network_residual_conv, buffer) \n",
    "print(buffer.tell()) \n",
    "\n",
    "del Network_residual_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a40fd-d126-4b7e-bdb5-b53f2dfdc662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "model_conv_residual_basic = torch.load(buffer)\n",
    "\n",
    "conv_res_residual_basic = train_model(model_conv_residual_basic, 0.001, Train_basic_conv, Test_conv, 20, 0.5, 'convolutional residual basic', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6017592-3e20-475b-9c0a-c5a765b39ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "model_conv_residual_balanced = torch.load(buffer)\n",
    "\n",
    "conv_res_residual_balanced = train_model(model_conv_residual_balanced, 0.001, Train_balanced_conv, Test_conv, 20, 0.5, 'convolutional residual balanced', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2cce3-4ad5-4483-b737-214b4486d1ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer.seek(0)\n",
    "model_conv_residual_balanced_MLSMOTE = torch.load(buffer)\n",
    "\n",
    "conv_res_residual_balanced_MLSMOTE = train_model(model_conv_residual_balanced_MLSMOTE, 0.001, Train_balanced_MLSMOTE_conv, Test_conv, 20, 0.5, 'convolutional residual balanced SMOTE', \n",
    "                  metrics_arr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f88612-b264-4b26-a3b6-eb5930181aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "del buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e43924a-774c-4941-9fc4-a0e47dc5bb88",
   "metadata": {},
   "source": [
    "## Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f602b84-8f84-4c1f-a73e-cbfbff4fa1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_data = np.expand_dims(Y_test1.sum(axis=0).int(), axis=0)\n",
    "columns = [\"Количество примеров\"]\n",
    "col1 = [\"GRU\", \"Свёрточная базовая\", \"Свёрточная продвинутая\"]\n",
    "col2 = [\"Исходная выборка\", \"Авnорская балансировка\", \"MLSMOTE\"]\n",
    "\n",
    "results = [\n",
    "    recurent_res_basic, recurent_res_balanced, recurent_res_balanced_MLSMOTE,    \n",
    "    conv_res_basic, conv_res_balanced, model_conv_balanced_MLSMOTE, \n",
    "    conv_res_residual_basic, conv_res_residual_balanced, conv_res_residual_balanced_MLSMOTE\n",
    "          ]\n",
    "\n",
    "for metric_num in range(len(metrics_arr)):\n",
    "    for res in results:\n",
    "        df_res_data = np.concatenate((df_res_data, [res[3][-1, metric_num]]), axis=0)\n",
    "\n",
    "for metric in metrics_arr:\n",
    "    for c1 in col1:\n",
    "        for c2 in col2:\n",
    "            columns += [c1 + ' ' + c2 + ' '  + metric.__name__]\n",
    "\n",
    "\n",
    "res_df = pd.DataFrame(data=np.transpose(df_res_data), index=list(diagnose_set), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2c06c-1348-49a4-9d52-5425c07d88ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.precision = 3\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7430df7-e8e3-4d1e-a396-1674c54c5050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
